

















# Import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')





X_train = pd.read_csv("Training_set_values.csv")
y_train = pd.read_csv("Trainig_test_labels.csv")
X_test = pd.read_csv("test_set_values.csv") 

print(X_train.shape, y_train.shape, X_test.shape)





print(X_train.info()) # Displays the data types of each column. 
print(X_train.describe(include='all').T) # Shows descriptive statistics for all columns (including categorical columns) 
print(y_train['status_group'].value_counts()) # Counts how many samples belong to each class in the target column (status_group)









# Check missing values
missing = X_train.isnull().sum().sort_values(ascending=False)
print(missing[missing > 0])






       #(X_train)
# Fill missing numerical values
X_train['construction_year'].replace(0, pd.NA, inplace=True)
X_train['construction_year'].fillna(X_train['construction_year'].median(), inplace=True)

# Fill categorical nulls with 'unknown'
cat_cols = X_train.select_dtypes(include='object').columns
X_train[cat_cols] = X_train[cat_cols].fillna("unknown")

X_train[cat_cols]


       # (X_test)
# Fill missing numerical values 
X_test['construction_year'].replace(0, pd.NA, inplace=True)
X_test['construction_year'].fillna(X_test['construction_year'].median(), inplace=True)

# Fill categorical nulls with 'unknown'
cat_cols = X_test.select_dtypes(include='object').columns
X_test[cat_cols] = X_train[cat_cols].fillna("unknown") 

X_test[cat_cols]






import pandas as pd 
# Reload the data set to retrieve "construction_year" column 

X_train = pd.read_csv("Training_set_values.csv")
X_test = pd.read_csv("Test_set_values.csv")

# Treat invalid years (0) as missing
X_train['construction_year'].replace(0, pd.NA, inplace=True)
X_test['construction_year'].replace(0, pd.NA, inplace=True)

# Fill in missing years with a reasonable estimate
X_train['construction_year'].fillna(X_train['construction_year'].median(), inplace=True)
X_test['construction_year'].fillna(X_test['construction_year'].median(), inplace=True)



# create pump_age
X_train['pump_age'] = 2025 - X_train['construction_year']
X_test['pump_age'] = 2025 - X_test['construction_year'] 



X_train['pump_age']
X_test['pump_age'] 





# Step 1: Combine rare categories in categorical features (threshold: <1% frequency)
def combine_rare_categories(df, threshold=0.01):
    for col in df.select_dtypes(include='object').columns:
        freq = df[col].value_counts(normalize=True)
        rare_labels = freq[freq < threshold].index
        df[col] = df[col].replace(rare_labels, 'other')
    return df

# Apply to both train and test
X_train = combine_rare_categories(X_train)
X_test = combine_rare_categories(X_test)

# Step 2: Concatenate and one-hot encode (now with fewer categories)
combined = pd.concat([X_train, X_test], axis=0)
combined = pd.get_dummies(combined)

# Step 3: Split back
X_train_cleaned = combined.iloc[:X_train.shape[0], :]
X_test_cleaned = combined.iloc[X_train.shape[0]:, :]






# Combine train and test to encode consistently
combined = pd.concat([X_train, X_test], axis=0)
combined = pd.get_dummies(combined)

# Split back
X_train_cleaned = combined.iloc[:X_train.shape[0], :]
X_test_cleaned = combined.iloc[X_train.shape[0]:, :]

X_train_cleaned
X_test_cleaned











from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train['status_group'])

y_train_encoded











from sklearn.model_selection import train_test_split

X_tr, X_val, y_tr, y_val = train_test_split(
    X_train_cleaned, y_train_encoded, test_size=0.2, random_state=42, stratify=y_train_encoded
)
 





from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# Train the model 
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_tr, y_tr)

# Predict
y_pred = log_reg.predict(X_val)

# Evaluate
print("Logistic Regression Accuracy:", accuracy_score(y_val, y_pred))
print(classification_report(y_val, y_pred))









from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)
rf.fit(X_tr, y_tr)
y_pred_rf = rf.predict(X_val)

print("Random Forest Accuracy:", accuracy_score(y_val, y_pred_rf))
print(classification_report(y_val, y_pred_rf))









from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import train_test_split



# Split the data
X_tr, X_val, y_tr, y_val = train_test_split(X_train_cleaned, y_train_encoded, test_size=0.2, random_state=42, stratify=y_train_encoded)

# Apply SMOTE to training set 
smote = SMOTE(random_state=42)
X_tr_balanced, y_tr_balanced = smote.fit_resample(X_tr, y_tr)



# Train Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_tr_balanced, y_tr_balanced) 

# Evaluate
y_pred = rf.predict(X_val)
print("Random Forest Accuracy:", accuracy_score(y_val, y_pred))
print(classification_report(y_val, y_pred))








# introduce new library 
from sklearn.model_selection import GridSearchCV


# Define parameter grid 
param_grid = {
    'n_estimators': [100, 150],           # number of trees
    'max_depth': [10, None],          # depth of each tree
    'min_samples_split': [2, 5,],      # min samples to split a node
    'min_samples_leaf': [2, 5],        # min samples at leaf node
     
}



#  Initialize the mode 
rf = RandomForestClassifier(random_state=42)

# Set up GridSearchCV 
grid_search = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=3,               # 3-fold cross-validation
    n_jobs=-1,          # use all CPU cores
    verbose=2,
    scoring='f1_macro'  # for imbalanced multiclass
)
grid_search


# Fit to the SMOTE-balanced data 
grid_search.fit(X_tr, y_tr)

# Best parameters 
print("Best parameters:", grid_search.best_params_)
best_rf = grid_search.best_estimator_



# Evaluate 
y_pred = best_rf.predict(X_val)

from sklearn.metrics import classification_report, accuracy_score
print("Accuracy:", accuracy_score(y_val, y_pred))
print(classification_report(y_val, y_pred))


















